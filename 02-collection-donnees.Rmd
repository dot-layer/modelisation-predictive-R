
# Collecte de données

``` {r, echo = FALSE}
library(magrittr)
```

La collecte de données est le point de départ de la chaîne. Les données étant quelque peu la matière première du produit que nous nous apprêtons à bâtir, plusieurs considérations influenceront leur collecte. 

Pour illustrer le tout, prétendons pour l'instant que le produit qui nous intéresse est en fait une table. La construction d'une table nécessite évidemment de considérer la quantité, la qualité et le prix de la matière première. Toutefois, plusieurs considérations additionnelles viennent s'ajouter quand on s'intéresse à la provenance, au processus de récolte ou de transformation et à l'entreposage des matériaux.

Les mêmes considérations peuvent s'appliquer dans notre contexte. La section suivante aura donc pour objectif d'aborder ces considérations, tout en soumettant les données de notre atelier à celles-ci.

Idées :
- Toujours relier la théorie au dataset bixi (pourquoi on l'a pris, etc.)

## Liste des données potentielles pour notre atelier

La première étape d'un déploiement de modèle prédictif est de dresser la liste des sources de données potentielles.

Les plus expérimentés auront déjà en tête la majorité des considérations couvertes dans ce chapitre au moment de dresser leur liste. Dans notre cas, permettons-nous de lister naïvement un grand nombre de sources. Chaque considération à venir nous permettra de distiller notre sélection.

- http://donnees.ville.montreal.qc.ca/dataset?q=velo
    - BIXI - Historique des déplacements
    - BIXI - État des stations
    - Arceaux à vélos
    - Piste Cyclable
    - Vélos - comptage sur les pistes cyclables
- Données météorologiques
    - température
    - précipitations
    - vent
- Données géographiques
    - découpage des quartiers
    - pentes
    - cours d'eau
    - pistes cyclables
- Données démographiques
    - densité
    - distribution du revenu
    - distribution de l'âge
- Données individuelles
    - adresse
    - statut résident/visiteur
    - lieu de travail
- Historique de trajets par individu


Tel que discuté dans l'introduction, notre ouvrage est linéaire, donc le processus de sélection de données se fera d'un seul trait. Bien entendu, dans un projet réel, il est possible que certains aspects nous échappent au début, et que nous devions réajuster le tir dans les phases subséquentes.

> [STOP] demander aux participants leurs idées de dataset intéressant

## Considérations {#considerations}

Comme nous pouvons le constater, la liste de données potentielles est sans limites. Les considérations qui suivent auront pour objectif de nous exposer les limites de certaines sources, pour ensuite identifier celle(s) qui sont le plus adaptées à notre problématique.

### Volume
On parlera de volume plutôt que de quantité.

Nous définissons le volume comme l'espace occupé par un objet sur disque ou en mémoire. Le volume d'une source de données sera donc le produit du nombre d'observations et de la richesse de chaque observation.

Si notre source de données contient beaucoup d'observations ou des observations riches, nous aurons donc des contraintes d'espace disque ou de mémoire. Bien que R soit parfois réputé lent ... [il n'en est rien](http://adv-r.had.co.nz/Performance.html)

mémoire vive (algos à batches)

#### Nombre d'observations
Sampling rate
Niveau d'aggrégation → perte potentielle d'information car nous pouvons toujours faire l'aggrégation nous-même
base / dplyr / data.table [autre section?](https://stackoverflow.com/questions/21435339/data-table-vs-dplyr-can-one-do-something-well-the-other-cant-or-does-poorly)

> [Sté] aggrégation fait partie du pre-processing mais absent dans notre ouvrage. Je couvre? Je mentionne? J'ignore? Ex. on aurait pu aggréger par ID du client si on avait eu un ID de client

#### Format
La richesse d'une observation est définie par son format.

Nous définissons le format comme la façon dont est représentée une donnée. C'est entre autres ce qui va déterminer la richesse de chaque donnée. On peut alors placer les données sur un continuum de structuré à non-structuré. Certaines définitions considèrent que les données semi-structurées sont une classe à part... nous nous contenterons de dire qu'il existe un continuum de degrés d'organisation des données.

##### Données structurées
organisées en base de données relationnelles constituées de tables disposées en colonnes et rangées. On utilise un modèle de données  
SQL=Structured Query Language  
oracle

##### Données non-structurées
les données non-structurées sont celles qui ne sont pas organisées à l'aide d'un modèle de données. Une base de données non-structurée peut donc contenir à la fois du texte, des images, du son ou une combinaison (ex. vidéo).  
JSON  
XML

### Qualité
Plusieurs aspects peuvent influencer notre perception de qualité d'une source de donnée, tels que son organisation, sa documentation, son pouvoir prédictif, sa fréquence, la capacité de faire des jointures et le biais. Encore une fois, notons que les étapes futures peuvent invalider une source de données. Par exemple, le pouvoir prédictif est difficile à évaluer sans un premier modèle naïf au moins.

#### organisation et documentation des données
Ces 3 sources de données contiennent la même information. Avec laquelle préfériez-vous travailler *a priori*?

```{r, echo = FALSE}
maintenant <- lubridate::now()
kable_settings <- function(x) kableExtra::kable_styling(x, bootstrap_options = "striped", full_width = F, position = "left")

knitr::kable(tibble::data_frame(dt = maintenant, temps = 0.5/24)) %>%
  kable_settings
knitr::kable(tibble::data_frame(Date = maintenant, `Durée (minutes)` = 30)) %>%
  kable_settings
knitr::kable(tibble::data_frame(`Date début` = maintenant, `Date fin` = maintenant + 30 * 60)) %>%
  kable_settings
```

Évidemment, nous préférerons, dans l'ordre, 3, 2, 1. Bien que cet exemple soit simpliste et fictif, il illustre tout de même que des éléments tels que le nom, la structure ou la documentation des données influenceront notre capacité à maximiser notre utilisation de ces données.

#### complexité
Rapport coût-bénéfice
Bénéfice : A-t-on déjà des bons proxys?
Coût : complexité additionnelle

#### pouvoir prédictif
Garbage In Garbage Out

#### fréquence
À quelle fréquence la source de données est-elle mise à jour? À quelle fréquence l'information sous-jacente à la source de données change-t-elle? De quelle précision aurons-nous besoin? Est-ce que note prédiction dépend de données instantanées ou intemporelles?

```{r, warning = FALSE, echo = FALSE}
x <- tibble::data_frame(`Mise à jour fréquente` = c("Données météorologiques", "????"),
                        `Mise à jour rare` = c("????", "Données géographiques"))
rownames(x) <- c("Changements fréquents", "Changements rares")

knitr::kable(x) %>%
  kable_settings
```

météo demande d'avoir un live feed, alors que données géographiques peuvent être figées dans le temps

#### Merge
facilité/pertinence de la clé, etc. Exemple : historique des déplacements du client ... pas dispo. Données ouvertes Montréal : ok peut-être...

http://donnees.ville.montreal.qc.ca/dataset/pistes-cyclables
http://donnees.ville.montreal.qc.ca/dataset/geobase


#### Biais
socio-démographique, etc.
dès que des données personnelles sont en jeu
dès que des proxys de données personnelles sont en jeu
aggravé lorsque les prédictions ont un impact réel sur la vie des gens

### Accessibilité
Dans notre parallèle avec la table, le coût était la troisième considération principale. Dans le cas des données, nous allons plutôt parler d'accessibilité, dont le coût est une des composantes.

#### coût (gratuit?)
Plusieurs (#sources) gratuites

#### Sources {#sources}
- https://www.kaggle.com/datasets
- https://toolbox.google.com/datasetsearch
- https://www.reddit.com/r/datasets
- https://archive.ics.uci.edu/ml/index.php?fbclid=IwAR09F5grBOTCr1SS4v8gONEYeqk0DqqWpPdt1blmYF9ucZkhsQeM5T0E7ew
- http://donnees.ville.montreal.qc.ca/

#### Lecture de fichiers
data.table (fread)
readr
readxl
xlsx

#### Connexion à une base de données
sqldf
dbplyr
rio
DBI
odbc
RMySQL, RPostgresSQL, RSQLite
foreign
haven
https://db.rstudio.com/

#### Web Scraping (légalité/api/service/timeouts)
XML
xml2
curl
httr
rvest
downloader
jsonlite
googlesheets

#### Stabilité
- Évolution anticipée du format des données (pas de contrôle sur la source de données? → copier les données freezées. Le modèle a encore de la valeur)
- Pourquoi on va utiliser les shapefiles comme exemple de merge (statique, etc.)

#### sécurité

## Références
